import math

# Define the dataset as a list of tuples (outlook, temperature, humidity, wind, play_tennis)
dataset = [
    ('Sunny', 'Hot', 'High', 'Weak', 'No'),
    ('Sunny', 'Hot', 'High', 'Strong', 'No'),
    ('Overcast', 'Hot', 'High', 'Weak', 'Yes'),
    ('Rain', 'Mild', 'High', 'Weak', 'Yes'),
    ('Rain', 'Cool', 'Normal', 'Weak', 'Yes'),
    ('Rain', 'Cool', 'Normal', 'Strong', 'No'),
    ('Overcast', 'Cool', 'Normal', 'Strong', 'Yes'),
    ('Sunny', 'Mild', 'High', 'Weak', 'No'),
    ('Sunny', 'Cool', 'Normal', 'Weak', 'Yes'),
    ('Rain', 'Mild', 'Normal', 'Weak', 'Yes')
]

# Define the attributes (Outlook, Temperature, Humidity, Wind)
attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']

# Function to calculate entropy
def entropy(data):
    total = len(data)
    values = set([entry[-1] for entry in data])  # Extract target column
    entropy_value = 0.0

    for value in values:
        prob = sum(1 for entry in data if entry[-1] == value) / total
        entropy_value -= prob * math.log2(prob)

    return entropy_value

# Function to calculate information gain
def information_gain(data, attribute_index):
    total_entropy = entropy(data)
    attribute_values = set([entry[attribute_index] for entry in data])
    
    weighted_entropy = 0.0
    for value in attribute_values:
        subset = [entry for entry in data if entry[attribute_index] == value]
        weighted_entropy += (len(subset) / len(data)) * entropy(subset)

    return total_entropy - weighted_entropy

# Function to select the best attribute based on information gain
def best_attribute(data, attributes):
    best_gain = -1
    best_attr_index = -1

    for index, attribute in enumerate(attributes):
        gain = information_gain(data, index)
        if gain > best_gain:
            best_gain = gain
            best_attr_index = index

    return best_attr_index

# Function to build the decision tree
def id3(data, attributes):
    # If all instances have the same label, return a leaf node
    if len(set([entry[-1] for entry in data])) == 1:
        return {'label': data[0][-1]}

    # If no attributes left to split, return the most common label
    if not attributes:
        most_common = max(set([entry[-1] for entry in data]), key=[entry[-1] for entry in data].count)
        return {'label': most_common}

    # Select the best attribute to split
    best_attr_index = best_attribute(data, attributes)
    best_attr = attributes[best_attr_index]

    # Create a root node for the tree
    tree = {'attribute': best_attr, 'branches': {}}
    
    # Get all unique values for the best attribute
    attribute_values = set([entry[best_attr_index] for entry in data])

    # Recurse on each subset of the data
    for value in attribute_values:
        subset = [entry for entry in data if entry[best_attr_index] == value]
        subtree = id3(subset, [attr for i, attr in enumerate(attributes) if i != best_attr_index])
        tree['branches'][value] = subtree

    return tree

# Function to print the decision tree
def print_tree(tree, indent=''):
    if 'label' in tree:
        print(indent + 'Label: ' + tree['label'])
    else:
        print(indent + tree['attribute'])
        for value, subtree in tree['branches'].items():
            print(indent + '  ' + str(value) + ':')
            print_tree(subtree, indent + '    ')

# Create the decision tree using ID3 algorithm
tree = id3(dataset, attributes)

# Print the decision tree
print_tree(tree)
